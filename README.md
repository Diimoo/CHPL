# Brain-Like Visual Learning

Official code for the paper:
> **Brain-Like Visual Learning Shows Emergent Developmental Bias and Implicit Regularization**  
> Ahmed Trabelsi  
> 

## Overview

We investigate whether brain-inspired plasticity rules can achieve cross-modal learning from scratch in a controlled synthetic environment. Using reconstruction-based predictive coding and Hebbian consolidation, our model (CHPL) learns to bind visual shapes with linguistic labels without supervised classification.

### Key Findings

1. **Reduced Color Bias**: Brain-inspired learning (1.23 ± 0.09) shows significantly less categorization bias than supervised backpropagation (2.25 ± 0.48), $p < 0.00001$.
2. **Emergent Development**: Naturally recapitulates the infant color-to-shape bias trajectory over 100 epochs (neutral → peak color bias → reduction).
3. **Mechanistic Insights**: ATL consolidation is identified as critical for cross-modal alignment (+41% improvement, $d=1.44$).

## Installation

```bash
pip install -r requirements.txt
```

Requires Python 3.8+ and PyTorch 2.0+ (CUDA recommended for faster validation).

## Quick Start

The model requires three-phase training (visual reconstruction → language alignment → cross-modal binding). For a complete working example, see `validation_study.py`.

```python
from brain_crossmodal_learner import BrainCrossModalLearner
from synthetic_environment import create_stimulus
from brain_crossmodal_learner import DEVICE
import torch

# Create model
model = BrainCrossModalLearner(feature_dim=64, n_concepts=100)

# Example inference (after training)
img = create_stimulus(shape='circle', color='red', size='medium')
img_t = torch.tensor(img, dtype=torch.float32).to(DEVICE)
with torch.no_grad():
    vis_features = model.visual(img_t)
    _, concept_idx = model.atl.activate(vis_features, 'visual')
print(f"ATL concept: {concept_idx}")

# For full training pipeline, see validation_study.py
```

## Reproducing Paper Results

### 1. Main Results (Ablation Study)
```bash
python validation_study.py
```
Runs 10 random seeds across 4 conditions (Full, No-Recon, No-Consol, Backprop).  
Results saved to `results/validation_results/`.

### 2. Developmental Trajectory
```bash
python extended_training.py
```
Tracks bias emergence over 100 epochs (3 seeds).  
Results saved to `results/extended_training_results/`.

### 3. Generate Figures
**Note:** Figure generation requires completed validation and trajectory runs.
```bash
# First ensure data exists
python validation_study.py      # Creates results/validation_results/
python extended_training.py     # Creates results/extended_training_results/

# Then generate figures
python generate_figures.py
```
Generates publication-quality figures:
- Paper figures (in `paper/`):
  - `figure1_ablation_results.pdf`
  - `figure2_developmental_trajectory.pdf`
  - `figure3_language_ablation.pdf`

## Project Structure

- `brain_crossmodal_learner.py`: Core model implementation (CHPL).
- `synthetic_environment.py`: Synthetic visual-linguistic stimulus generator.
- `validation_study.py`: Statistical validation script (10 seeds).
- `extended_training.py`: Long-term trajectory tracking (100 epochs).
- `generate_figures.py`: Figure generation script.
- `paper/`: LaTeX and Markdown drafts of the paper.
- `results/`: Output directory for validation and trajectory data (generated by running scripts).
- `docs/FINDINGS.md`: Summary of key scientific findings.

## Citation

```bibtex
@article{trabelsi2025brain,
  title={Brain-Like Visual Learning Shows Emergent Developmental Bias and Implicit Regularization},
  author={Trabelsi, Ahmed},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2025}
}
```

## License

MIT License - See LICENSE file for details.

## Acknowledgments

This work builds on principles from computational neuroscience and developmental psychology. The model architecture was inspired by the anterior temporal lobe's role in semantic memory consolidation.
